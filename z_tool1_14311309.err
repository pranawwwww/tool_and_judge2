ğŸ”— Found pyo3 bindings
ğŸ Found CPython 3.13 at /projects/bfdz/zluo8/tool_and_judge2/.venv/bin/python
    Finished `release` profile [optimized] target(s) in 1.05s
ğŸ“– Found type stub file at codebase_rs.pyi
ğŸ“¦ Built wheel for CPython 3.13 to /tmp/.tmprzyiOV/codebase_rs-0.1.0-cp313-cp313-linux_x86_64.whl
ğŸ›  Installed codebase_rs-0.1.0
/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/transformers/models/auto/tokenization_auto.py:1041: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[0;36m(EngineCore_DP0 pid=328030)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=328030)[0;0m Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:01<00:12,  1.75s/it]
[0;36m(EngineCore_DP0 pid=328030)[0;0m Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:03<00:10,  1.82s/it]
[0;36m(EngineCore_DP0 pid=328030)[0;0m Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:05<00:09,  1.84s/it]
[0;36m(EngineCore_DP0 pid=328030)[0;0m Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:07<00:07,  1.85s/it]
[0;36m(EngineCore_DP0 pid=328030)[0;0m Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:09<00:05,  1.85s/it]
[0;36m(EngineCore_DP0 pid=328030)[0;0m Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:11<00:03,  1.86s/it]
[0;36m(EngineCore_DP0 pid=328030)[0;0m Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:12<00:01,  1.57s/it]
[0;36m(EngineCore_DP0 pid=328030)[0;0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:13<00:00,  1.62s/it]
[0;36m(EngineCore_DP0 pid=328030)[0;0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:13<00:00,  1.72s/it]
[0;36m(EngineCore_DP0 pid=328030)[0;0m 
[0;36m(EngineCore_DP0 pid=328030)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 2/35 [00:00<00:02, 13.92it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|â–ˆâ–        | 4/35 [00:00<00:02, 14.63it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|â–ˆâ–‹        | 6/35 [00:00<00:01, 15.19it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|â–ˆâ–ˆâ–       | 8/35 [00:00<00:01, 15.75it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|â–ˆâ–ˆâ–Š       | 10/35 [00:00<00:01, 16.21it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:00<00:01, 16.51it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:00<00:01, 16.70it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:00<00:01, 16.87it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:01<00:00, 17.20it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:01<00:00, 17.53it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 22/35 [00:01<00:00, 14.87it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:01<00:00, 15.72it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 26/35 [00:01<00:00, 16.53it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:01<00:00, 17.20it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 30/35 [00:01<00:00, 17.84it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:01<00:00, 18.31it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:02<00:00, 18.13it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:02<00:00, 16.79it/s]
[0;36m(EngineCore_DP0 pid=328030)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   5%|â–Œ         | 1/19 [00:00<00:04,  3.98it/s]Capturing CUDA graphs (decode, FULL):  16%|â–ˆâ–Œ        | 3/19 [00:00<00:01,  9.63it/s]Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–‹       | 5/19 [00:00<00:01, 12.95it/s]Capturing CUDA graphs (decode, FULL):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [00:00<00:00, 15.04it/s]Capturing CUDA graphs (decode, FULL):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [00:00<00:00, 16.57it/s]Capturing CUDA graphs (decode, FULL):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 12/19 [00:00<00:00, 18.13it/s]Capturing CUDA graphs (decode, FULL):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [00:00<00:00, 19.38it/s]Capturing CUDA graphs (decode, FULL):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [00:01<00:00, 20.50it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:01<00:00, 16.93it/s]
[2025-12-23 00:30:37] INFO _client.py:1740: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-23 00:30:38] INFO _client.py:1740: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-23 00:30:38] INFO _client.py:1740: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-23 00:30:38] INFO _client.py:1740: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-23 00:30:39] INFO _client.py:1740: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-23 00:30:39] INFO _client.py:1740: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-23 00:30:39] INFO _client.py:1740: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-23 00:30:39] INFO _client.py:1740: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-23 00:30:39] INFO _client.py:1740: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-23 00:30:39] INFO _client.py:1740: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-23 00:30:39] INFO _client.py:1740: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-23 00:30:39] INFO _client.py:1740: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-23 00:30:39] INFO _client.py:1740: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-23 00:30:39] INFO _client.py:1740: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-23 00:30:40] INFO _client.py:1740: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-23 00:30:40] INFO _client.py:1740: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-23 00:30:40] INFO _client.py:1740: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-23 00:30:40] INFO _client.py:1740: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-23 00:30:40] INFO _client.py:1740: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-23 00:30:41] INFO _client.py:1740: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-23 00:30:42] INFO _client.py:1740: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-23 00:30:42] INFO _client.py:1740: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-23 00:30:43] INFO _client.py:1740: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-23 00:30:49] INFO _client.py:1740: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
Exception ignored in: <function AsyncLLM.__del__ at 0x7fef39dc74c0>
Traceback (most recent call last):
  File "/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/vllm/v1/engine/async_llm.py", line 263, in __del__
  File "/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/vllm/v1/engine/async_llm.py", line 268, in shutdown
TypeError: 'NoneType' object is not callable
