ðŸ”— Found pyo3 bindings
ðŸ Found CPython 3.13 at /projects/bfdz/zluo8/tool_and_judge2/.venv/bin/python
warning: unused variable: `configs`
  --> src/lib.rs:15:29
   |
15 |     async fn tool_run_async(configs: Py<PyList>, num_gpus: usize) {
   |                             ^^^^^^^ help: if this is intentional, prefix it with an underscore: `_configs`
   |
   = note: `#[warn(unused_variables)]` on by default

warning: unused variable: `num_gpus`
  --> src/lib.rs:15:50
   |
15 |     async fn tool_run_async(configs: Py<PyList>, num_gpus: usize) {
   |                                                  ^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_num_gpus`

warning: `codebase_rs` (lib) generated 2 warnings
    Finished `release` profile [optimized] target(s) in 0.90s
ðŸ“– Found type stub file at codebase_rs.pyi
ðŸ“¦ Built wheel for CPython 3.13 to /tmp/.tmpzDD5Tm/codebase_rs-0.1.0-cp313-cp313-linux_x86_64.whl
ðŸ›  Installed codebase_rs-0.1.0
/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/transformers/models/auto/tokenization_auto.py:1041: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
Traceback (most recent call last):
  File "/projects/bfdz/zluo8/tool_and_judge2/judge.py", line 219, in <module>
    model, tokenizer = create_huggingface_backend(model_name, batch_size)
                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
  File "/projects/bfdz/zluo8/tool_and_judge2/src_py/huggingface_backend.py", line 39, in create_huggingface_backend
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
    ...<4 lines>...
        max_memory={0: "40GB"},  # Limit to prevent offloading to CPU
    )
  File "/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py", line 604, in from_pretrained
    return model_class.from_pretrained(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py", line 277, in _wrapper
    return func(*args, **kwargs)
  File "/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py", line 5048, in from_pretrained
    ) = cls._load_pretrained_model(
        ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        model,
        ^^^^^^
    ...<12 lines>...
        weights_only=weights_only,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py", line 5432, in _load_pretrained_model
    caching_allocator_warmup(model, expanded_device_map, hf_quantizer)
    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py", line 6090, in caching_allocator_warmup
    device_memory = torch_accelerator_module.mem_get_info(index)[0]
                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^
  File "/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/torch/cuda/memory.py", line 838, in mem_get_info
    return torch.cuda.cudart().cudaMemGetInfo(device)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^
torch.AcceleratorError: CUDA error: CUDA-capable device(s) is/are busy or unavailable
Search for `cudaErrorDevicesUnavailable' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

